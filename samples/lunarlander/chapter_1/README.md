# Lunar Lander VP Link model, Chapter 1

If you are here, you should have completed the [instructions](../README.md) in the documentation
in the root directory of this sample.

That means you have a SIM called "lunarlander", and are ready to create a brain.

## Let's start with a simple brain

If you have not created a brain yet, go to the bonsain main page and create an empty brain.
Call it {your_initials}\_LunarLander.  This way, you can keep your brains separate from others
that may also be using your bonsai workspace.

Copy the [lunarlander_ch1.ink](lunarlander_ch1.ink) code and paste it into your brain.
Before we train the brain, let's take a look at what we have.

## Inkling Discussion

### The SimState
Lines 12-22 define the SimState. This is the structure that the SIM will pass back
to bonsai in reponse to the SimActions that bonsai calculates.  This SimState is
determined by the SIM, and is described in the interface.json in the sim.zip file.

### The SimAction
Lines 26-31 define the SimAction. This is the structure that bonsa is responsible
for calculating.  The SimAction is transmitted to the SIM, which then calculates a
SimState for bonsai.  Round and round the SimAction and SimState go until the 
brain is trained.

### The SimConfig
Lines 35-52 define the SimConfig. This is the structure that is sent to the SIM
at the start of each training episode.  The values of the SimConfig are set by
lesson/scenario sections in your Inkling code.

### Constants
Lines 54-61 define some constants so we do not need to put "magic" numbers
throughout the code.  While is seems easier to just put in a value of 0.1 for the
VP Link time step, anybody who reads your code one month from now (including you)
will appreciate the use of named constants instead of bare numbers.

### Simulator
Lines 64-67 define the simulator to use to train this brain.  If you did not call
your SIM "lunarlander", this is the place to change the package to match your SIM
name.

### Concepts
The "Land" concept starts at line 74.  As we just have a single concept in this
brain, we use "input" as the input parameter, and SimAction as the type of value
that is generated by this concept.   The curriculum object 
defines the Trainig, the Goals, and the Lessons.

#### Curriculum Training
Three parameters of the training have been set.  The EpisodeIterationLimit places a maximium
value on how long a training episode will take.  In this case, we use an expression to 
calculate the number of training iterations to use as the maximum.  Figure out how long 
bonsai _should_ take to solve the problem. Then divide by the number of seconds/iteration
the SIM will do.  If you specify a time that is too short, bonsai won't get a chance to 
solve the problem fully.  If you specify a time that is too long, then you will likely waste 
training iterations where bonsai has gone off into the weeds and is not learning anything.

The TotalIterationLimit prevents the entire training from consuming the world's compute 
power.  It places a limit on the number of iterations to do before completely giving up.
This is typically a number like 20-50 million.

The NoProgressIterationLimit defines when bonsai decides that the training for a
particular Lesson is done.  If this is too small, then bonsai might decide that the
training for that lesson is complete because there has been no progress in X iterations,
even though the overall progress towards success is still way less than 100%. In this 
case, the training would be stopped prematurely before other alternatives were explored
by bonsai.  If the limit is too long, then once bonsai gets very close to the 100% success
rate, it will still try for that many iterations to try to improve the result.  Numbers
from 200k to 1 million are common.

#### Curriculum Goals
Lines 88-100 are the most important thing that you as "the human" can specify.
These are what determines if bonsai is doing a good job or not.  In this case we have
set out the following goals.

* LandOnGround:  We want the lander to reach the ground.  Once it does, the training
session is over.  The "reach" statement will stop the training once this occurs. Note
that the Goal.RangeBelow(0.0) includes the value of zero in the range.  So once the
lander is on the ground, we are done.

* LandSlowly:  This is something we want the brain to do all the time, so we use a 
"drive" statement. In this case we want the brain to keep the descent rate (y_velocity)
between -0.5 and 0.0.

* WithinFlags:  We want the brain to keep the lander between the flags, so the x_position
is between -2.0 and 2.0 meters.  (The target landing point is at x=0, y=0).

* FlyingIntoSpace:  We want to avoid this, so naturally we use the "avoid" statement.
This tells bonsai it has done a bad thing.  In addition, the training episode is terminated,
so bonsai does not spend more time doing more bad things.  Avoid statements are 
very important to limit wasted training time.  Otherwise it is very likely the brain
would continue the training in some region of the SimState we do not care about.   In this
case, there is really no benefit to learn to go up to 50m above the flags.  

* HardLanding: This tells bonsai to avoid a situation which is bounded by more than one
variable.  In this case, getting inside the "box" of being very close to landing and 
with a large downward velocity is bad.

#### Curriculum Lessons
Lessons give the brain some guidance on how to train itself.  Imagine you are learning 
to swim.  Jumping off a boat into the ocean may not be the ideal way to start.  You 
might start by wading into the shallow end of a pool and learning how to float.  Lessons
do this for a brain.  They set a set of parameters that the brain will use while it is 
training.  Lessons are tackled in order.  Once a lesson has been mastered, the brain 
will automatically move to the next lesson.  Once all the lessons have been learned,
the concept is deemed "trained".

## Give it a try

Click on the concept to be trained (there is only one, I know), and then click the green train button to start the training.

![](ClickLandToTrain.png)

You will find this sample takes a very, very long time to train. It might not even train
at all before it reaches one of the training limits.  That's ok, it's why we have Chapter 2.
In Chapter 2, you will learn how to help the brain learn.

## Further investigations

This was a simple brain just to get us going with brain training.  You might want to 
explore some of these questions on your own.

*  How effective is the brain training in a single-concept vs. multi-concept architecture?
*  Can you architect a reward function that works as good as or better than the Goals?
*  Can a brain that is trained to work in Earth gravity also effectively land the lander
with Moon gravity?  This is a common question that relates to the sim-to-real gap.
